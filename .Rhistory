#https://www.xpdfreader.com/download.html
#https://stackoverflow.com/questions/21445659/use-r-to-convert-pdf-files-to-text-files-for-text-mining
# folder with 1000s of PDFs
# dest <- "C:\\Users\\lfortini\\Downloads\\to read new\\0_to_extract_text"
dest <- "D:\\misc\\to read new\\0_to_extract_text"
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
# convert each PDF file that is named in the vector into a text file
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('"C:/Program Files/xpdf/bin64/pdftotext.exe"',
paste0('"', i, '"')), wait = FALSE) )
}
####
#simplify text
dest <- "D:/misc/to read new/2_to_simplify/"
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = ".txt",  full.names = TRUE)
myfiles=myfiles[grep("simplified.txt", myfiles, invert = T)]
myfile=myfiles[3]
myfile
cat("doing ", myfile, "\n")
library(readtext)
#text_file_name="C:/Users/lfortini/Downloads/to read new/alberto et al 2013 to_simplify.txt"
#file_name="Fletcher et al. - 2019 - Invasive plants negatively impact native, but not .txt"
#text_file_name=paste0("D:/misc/to read new/2_to_simplify/", file_name)
text_file_name=myfile
to_simplify=readtext(text_file_name, encoding = "UTF-8")
#View(to_simplify)
text_string=to_simplify[1,2]
text_string=as.character(text_string)
text_string=gsub(pattern = "&", replacement="and", x = text_string) #ssml character
text_string=gsub(pattern = "'", replacement="&apos;", x = text_string) #ssml character
text_string=gsub(pattern = '"', replacement='', x = text_string) #ssml character
text_string=gsub(pattern = "and, ", replacement="and ", x = text_string)
text_string=gsub(pattern = "-\n", replacement="", x = text_string)
text_string=gsub(pattern = "'", replacement="", x = text_string) #different okina
text_string=gsub(pattern = " >", replacement=" greater than", x = text_string) #different okina
text_string=gsub(pattern = " >=", replacement=" greater or equal than", x = text_string) #different okina
text_string=gsub(pattern = "<", replacement=" lesser than", x = text_string) #different okina
text_string=gsub(pattern = "<=", replacement=" lesser or equal than", x = text_string) #different okina
#text_string=gsub(pattern="[\r\n]", replacement="<break></break>", text_string)
if (remove_line_returns) text_string=gsub(pattern = "\n", replacement=" ", x = text_string)
if (remove_parenthesis) text_string=gsub("\\([^()]*\\)", "", x = text_string) #remove parenthesis
if (remove_numbers_at_end_of_sentence) text_string2= gsub("[0-9]. $", ". ", text_string)
text_string=gsub("\\[[^()]*\\]", "", x = text_string) #remove brackets
#############################
#removing weird characters
# library(Unicode)
#to find unicode
#https://unicodelookup.com/#%C4%81/1 #look for hex code here 0x101 (000101)
# Unicode::as.u_char(utf8ToInt("'")) #get code and add to "\U00
text_string=gsub(pattern = "\U00FB01", replacement="fi", x = text_string)
text_string=gsub(pattern = "\U00FB03", replacement="ffi", x = text_string)
text_string=gsub(pattern = "\U00FB00", replacement="ff", x = text_string)
text_string=gsub(pattern = "\U00FB02", replacement="fl", x = text_string)
text_string=gsub(pattern = "\U000101", replacement="a", x = text_string) ##long a
text_string=gsub(pattern = "\U000113", replacement="e", x = text_string) #long e
text_string=gsub(pattern = "\U00012B", replacement="i", x = text_string) #long i
text_string=gsub(pattern = "\U00014D", replacement="o", x = text_string) #long o
text_string=gsub(pattern = "\U00016B", replacement="u", x = text_string) #long u
text_string=gsub(pattern = "\U000100", replacement="A", x = text_string) ##long a
text_string=gsub(pattern = "\U000112", replacement="E", x = text_string) #long e
text_string=gsub(pattern = "\U00012A", replacement="I", x = text_string) #long i
text_string=gsub(pattern = "\U00014C", replacement="O", x = text_string) #long o
text_string=gsub(pattern = "\U00016A", replacement="U", x = text_string) #long u
#############################
#remove all remaining non-ascii character
#https://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files
#r remove non-ascii from string
text_string=gsub("[^\u0001-\u007F]+|<U\\+\\w+>","", text_string)
#############################
#now add ssml tags
text_string=gsub(pattern = "#s#", replacement="<", x = text_string)
text_string=gsub(pattern = "#e#", replacement=">", x = text_string)
#text_string=gsub("o", " ", x = text_string)
#text_string8=iconv(text_string8, from = 'UTF-8', to = 'ASCII//TRANSLIT')
output_text_file_name=gsub(pattern=".txt", replacement="_simplified.txt", text_file_name)
write.table(text_string, output_text_file_name, sep=";", col.names=FALSE, quote=FALSE, row.names=FALSE) #, fileEncoding = "UTF-8")
folder_name="D:/projects/HIecoH/data_and_analysis/HIecoH_code/"
folder_name="D:/projects/Silversword matrix models/silversword_code/"
folder_name="D:/projects/Invasives_modeling/IS_V2_repo/"
#wd="D:/projects/Silversword matrix models/silversword_code/"
list_all_packages_in_repo=function(folder_name){
all_r_files=list.files(path = folder_name, pattern = "\\.r$|\\.R$", recursive = T, full.names = T)
all_r_files=all_r_files[grep(all_r_files, pattern = "old", invert = T)]
r_file=all_r_files[1]
all_packages=c()
for (r_file in all_r_files){
cat("doing ", r_file, "\n")
jnk=readLines(r_file)
i=2
if (length(jnk)>0){
for (i in c(1:length(jnk))){
line=jnk[i]
if (grepl(pattern="library\\(", x = line, )){
line=sub(".*library\\(", "", line)
package_name=sub("\\).*", "", line)
all_packages=c(all_packages, package_name)
}
if (grepl(pattern="requires\\(", x = line, )){
line=sub(".*requires\\(", "", line)
package_name=sub("\\).*", "", line)
all_packages=c(all_packages, package_name)
}
}
}
}
#unique
all_packages=sub(pattern='"', replacement="", x=all_packages)
all_packages=sub(pattern='\"', replacement="", x=all_packages)
all_packages=unique(all_packages)
#sub " and '
cat("Repository lists ", length(all_packages), " packages \n")
#check dependencies
#https://stackoverflow.com/questions/14645363/listing-r-package-dependencies-without-installing-packages
deps=tools::package_dependencies(packages = all_packages,
recursive = TRUE)
unique_dependencies=unique(unlist(deps))
# library(tidyverse)
# depend_DF=tibble(Package=names(deps),
#        data=map(deps, as_tibble)) %>%
#   unnest(data)
# unnest(deps)
# #View(depend_DF)
# unique_dependencies=unique(depend_DF$value)
# unique_dependencies
#what are base r packages?
x <- installed.packages()
x=as.data.frame(x[ !is.na(x[ ,"Priority"]), c("Package", "Priority") ])
base_packages=x$Package
base_packages=base_packages[base_packages %in% unique_dependencies]
#which are unique non-base packages?
unique_non_base_dependencies=unique_dependencies[!(unique_dependencies %in% base_packages)]
all_packages=data.frame(package=all_packages, type="listed")
base_packages=data.frame(package=base_packages, type="base_dependencies")
unique_non_base_dependencies=data.frame(package=unique_non_base_dependencies, type="non_base_dependencies")
all_results=rbind(all_packages, base_packages, unique_non_base_dependencies)
#all_results=list(all_packages, base_packages, unique_non_base_dependencies)
#names(all_results)=c("listed_packages", "base_dependencies", "non_base_dependencies")
#View(all_results)
all_installed=as.data.frame(installed.packages())
all_results=merge(all_results, all_installed, by.x = "package", by.y = "Package", all.x = T, all.y = F)
all_results=all_results[order(all_results$type),]
View(all_results)
return(all_results)
}
all_used_packages=list_all_packages_in_repo(folder_name)
View(all_used_packages)
install.packages("colorRamps")
### invasive species models source script ###
### scripts to build and run biomod2 sdms ###
### master code to use in sdm IS_analysis ###
# clear the environment, temp files, and all other variables
rm(list = ls())
##########################################
##### SET SOURCE LOCATIONS AND PATHS #####
##########################################
# set root path to source files
# rootDir<-"D:/projects/Invasives_modeling/Invasive_SDMs/"
# rootDir<-"E:/Invasive_SDMs/"
project_dirs=c("C:/Users/lkaiser-local/Desktop/Phase1_SDMs/", "E:/invasives_SDM/")
rootDir=project_dirs[min(which(dir.exists(project_dirs)))]
# set working directory to main analysis folder
setwd(rootDir)
# select name for project and create directory
project_run<-"global_notHI_models"
# set path of ongoing project run for all outputs
project_path<-paste0(rootDir, project_run, "/")
# create project folder path
dir.create(project_path, showWarnings = FALSE)
# location to save any extra or more specific outputs
outDir<-paste0(project_path, "outputs/")
# create output folder in project path
dir.create(outDir, showWarnings = FALSE)
# location of scripts and code
# codeDir<-paste0("D:/projects/Invasives_modeling/Invasive_SDMs/IS_V2/")
codeDirs=c("D:/projects/Invasives_modeling/IS_V2_repo/", paste0(rootDir, "IS_V2/")) #in order of priority
codeDir=codeDirs[min(which(dir.exists(codeDirs)))]
# location of all data
dataDir<-paste0(rootDir, "data/")
# all species data
allDir<-paste0(dataDir, "all_data/")
# Hawaii species data
hiDir<-paste0(dataDir, "hi_data/")
# excluding Hawaii species data
nohiDir<-paste0(dataDir, "no_hi_data/")
# location of map data and shapefiles
mapDirs<-c(paste0(dataDir, "map_data/"), "D:/data/")
mapDir<-mapDirs[min(which(dir.exists(mapDirs)))]
# location of bioclimatic variables
bioclims_dirs=c("D:/data/global_climate/wc2.1_30s_bio_simplified/", paste0(dataDir, "bioclim_vars/")) #in order of priority
bioclims_dir<-bioclims_dirs[min(which(dir.exists(bioclims_dirs)))]
# global bioclim variables V2.1 downloaded from worldclim.org (2020)
# current (2000) bioclimatic variables @ 5 arc-min (170 km2)
fitting_bios_global<-paste0(bioclims_dir, "all_baseline/current_30s/")
# current(2000) bioclimatic variables @ 10 arc-min (340 km2)
#current_proj_bios_global<-paste0(bioclims_dir, "all_baseline/current_10min/") #not used
# ### V2.1 NOT YET AVAILABLE ###
# # future (2100) bioclimatic variables @ 10 arc-min
# future_bios<-paste0(bioclims_dir, "all_future/future_10min/")
# updated HRCM bioclims_dir ***FOR HAWAII ONLY*** (2015)
# current updated bioclimatic variabels @ 125 m
fitting_bios_HIs<-c(paste0(bioclims_dir, "all_HRCM/current_250m_redone/"), "D:/data/climate_data/20201123_HRCM_NCAR_projections2/bioclims/")
fitting_bios_HI<-fitting_bios_HIs[min(which(dir.exists(fitting_bios_HIs)))]
# current updated bioclimatic variables @ 500 m
#changed to new recalc values (GLOBAL AND LOCAL SEEM TO HAVE DIFFERENT UNITS!)
current_proj_bios_HI<-fitting_bios_HI #paste0(bioclims_dir, "all_HRCM/current_250m_redone/")
# future updated bioclimatic variables @ 500 m
future_proj_bios_HIs<-c(paste0(bioclims_dir, "all_HRCM/future_500m/"), "D:/data/climate_data/20201123_HRCM_NCAR_projections2/bioclims/")
future_proj_bios_HI<-future_proj_bios_HIs[min(which(dir.exists(future_proj_bios_HIs)))]
# GLOBAL A: allDir, fitting_bios_global, current/future_proj_bios_HI
# GLOBAL B: nohiDir, fitting_bios_global, current/future_proj_bios_HI
# LOCAL: hiDir, fitting_bios_HI, current/future_proj_bios_HI
# WEIGHTED: hiDir, fitting_bios_HI, current/future_proj_bios_HI
# select current data and bioclims to use for model approach
baseData<-nohiDir                # baseline species data (scripts 1 & 2)
futureData<-hiDir              # future species data (scripts 3 & 5)
biofitRun<-fitting_bios_global     # for model fitting
biobaseRun<-current_proj_bios_HI    # for baseline projections
biofutureRun<-future_proj_bios_HI   # for future projections
##### GENERAL CONFIGURATIONS #####
##################################
# load necessary packages for this script
library("tools")
library("raster")
library("rgdal")
library("rworldmap")
library("rworldxtra")
library("maps")
library("maptools")
# set all_sp_nm = 'Clidemia_hirta' for testing and debugging
# list all 17 species names to be analyzed
all_sp_nm = c('Clidemia_hirta', 'Falcataria_moluccana', 'Hedychium_gardnerianum',
'Lantana_camara', 'Leucaena_leucocephala', 'Melinis_minutiflora',
'Morella_faya', 'Panicum_maximum',
'Passiflora_tarminiana', 'Pennisetum_clandestinum', 'Pennisetum_setaceum',
'Psidium_cattleianum', 'Setaria_palmifolia','Schinus_terebinthifolius',
'Cyathea_cooperi')
# NOTE: Cyathea cooperi is the species synonym for Sphaeropteris cooperi
# NOTE: Passiflora tarminiana is a species synonym of Passiflora mollisima
# # Phase 1 Select Species
# all_sp_nm = c('Clidemia_hirta', 'Lantana_camara', 'Pennisetum_clandestinum', 'Psidium_cattleianum')
# # large species files to run separately
# all_sp_nm = c('Miconia_calvescens', 'Ulex_europaeus')
# # create a subset of species to run if needed (run individually for global)
# sp_sub<-c('Clidemia_hirta')
# all_sp_nm<-sp_sub
# load map data for global extent
world_map<-getMap(resolution = "high")
# load shapefile for Hawaii extent
hawaii_map<-readOGR(paste0(mapDir, "Main_Hawaiian_Islands_simple3.shp"))
# set projection to be the same for all mapping
coordSys<-'+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0'
# add coordinate reference system for projections to be same
projection(world_map)<-coordSys
projection(hawaii_map)<-coordSys
# set map and scale (Hawaii or Global) to use for project run
map_scale<-"Global"
map_to_use<-world_map
# list global extent from world_map
all_ext<-extent(world_map)
# create local extent for Hawaii
hi_ext<-extent(hawaii_map)
# set crop extent for project run
crop_ext<-all_ext
#############################
##### MODELLING OPTIONS #####
#############################
# select BIOMOD2 models to run (ANN, CTA, FDA,  GAM, GBM, GLM, MARS, MAXENT, RF, SRE)
models_to_run = c("MAXENT.Phillips", "GBM")  #("GAM", "GBM", "GLM", "MAXENT", "RF")
# select model evaluation methods (KAPPA, ROC, TSS)
eval_stats = c("ROC", "KAPPA", "TSS")
# select environmental variables for models
env_var_files = c("bio1.tif", "bio7.tif", "bio12.tif", "bio15.tif")
# create vector with bioclimatic variable names without the file extension (.tif)
var_names<-unlist(file_path_sans_ext(env_var_files))
# choose whether to plot graphs (T) or not (F)
plot_graphs = TRUE
# plotting options depending on if server or not
useRasterDef = TRUE
interpolateDef = FALSE
# apply fixes for large (T) or small (F) models to solve memory issues (script 3b)
apply_biomod2_fixes = TRUE
# choose whether to overwrite past results (T) or not (F)
overwrite = FALSE
# select number of computer cores for processing (max = 32)
cpucores = 1
parallel_run = F
### MAIN SCRIPTS ###
# RUN 'T' FOR BOTH BASELINE AND FUTURE
# script 1: to run model fitting (T) or not (F)
EM_fitting = TRUE
# script 2: to run ensemble modeling (T) or not (F)
EM_ensemble = TRUE
# script 3: to project model results (T) or not (F)
EM_project = TRUE
### AUXILIARY SCRIPTS ###
# RUN 'T' FOR BASELINE, 'F' FOR FUTURE
# script 1a: to get variable importance and evaluation score (T) or not (F)
merge_var_imp_and_mod_eval = T
# script 1b: to graph evaluation scores/variable importance (T) or not(F)
model_fit_graphs = T
# script 2a: to graphy variable importance (T) or not (F)
create_response_curves = T
# RUN 'T' FOR BOTH BASELINE AND FUTURE
# script 4: to create raster files (T) or not (F)
raster_output_creation = T
# RUN 'F' FOR BASELINE, 'T' FOR FUTURE
# script 5: to map analog climates (T) or not (F)
create_analog_climate_map = F
# script 6: to calculate distribution shifts (T) or not (F)
calculate_distribution_shifts = F
# script 7: to create ensemble maps (T) or not (F)
species_ensemble_maps = F
##### SPECIFIC SCRIPT CONFIGURATIONS #####
##########################################
### EM_fitting (script 1)
# number of ensemble modeling evaluation runs (set to 10 for full runs)
NbRunEval = 10
# if the models should use response points weights or not
useYweights = FALSE
# consider PAs outside of a species climate envelope (T) or not (F)
PseudoAbs_outside_CE = FALSE
# set PA density that is equal to point density within surveyed areas
dens_PAs_outside_CE = 1
# select number of repetitions for PA selections
PA.nb.rep = 10
# select number of PAs to determine point density
number_of_PAs = 1 #Using 1 to 1, based on recommendation from https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2011.00172.x
#if less than 100, will use value to determine total PA as number_of_PAs * n of presences, if larger, will apply actual number
# candidate points to use only if PAs_outside_CE = F, if == 0, will use number_of_PAs
candidatePAperPA = PA.nb.rep*5  # overridden if PseudoAbs_outside_CE = T
# strategy for selecting PAs (disk, random, sre, user.defined)
PA.strategy = "random" #using random, as recommended by https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2011.00172.x
# set 100m equivalence distance from actual data points
equiv_100m = 0.0009430131
# set 20 km minimum distance from actual data points
PA.dist.min = 200*equiv_100m
# to run the full models (T) or not (F)
do.full.models = TRUE
### EM_ensemble (script 2)
# sets the minimum scores to exclude models when building ensembles
eval.metric.threshold = rep(0.5, length(eval_stats))
### EM_project (script 3)
# select baseline (1) or future (4) projections
baseline_or_future = 1
# choose to save clamping mask (T) or not (F)
clampingMask = FALSE
# to keep clamping Mask = T saved to hard drive (T) or not (F)
memory = TRUE
# assign projected climate data set for baseline scenario
if (baseline_or_future == 1) {
clim_data = biobaseRun
proj_nm = 'baseline'}
# assign projected climate data set for future scenario
if (baseline_or_future == 4) {
clim_data = biofutureRun
proj_nm = 'future'}
# temporary folder for files during processing to avoid memory errors
dir_for_temp_files<-paste0(rootDir, project_run, "/temp/", baseline_or_future, "/")
# conditions if applying fixes to BIOMOD2 (script 3b)
if (apply_biomod2_fixes) {
# name model run based on scenario
maxentWDtmp = paste0("maxentWDtmp_", baseline_or_future)
# create temporary directory file
dir.create(dir_for_temp_files, showWarnings = F, recursive = T)
}
### raster_output_creation (script 4)
# number of projections to create raster - 1 for baseline, 2 for both
projections_to_run = 1
# type of ensemble configurations for multiple species maps
spp_ensemble_type = "wmean"
# for raster creation and shifted calculations for mapping
spp_ensemble_eval_stats = c('ROC', 'TSS', 'KAPPA')
# for raster creation and shifted calculations
comp_projects = c('baseline', 'future')
# for raster creation
plot_spp_ensemble_CV = TRUE
# for raster creation for future runs only **depreciated**
masked_spp_ensemble_map = FALSE
### create_analog_climate_map (script 5)
#create_analog_climate_map configurations for baseline (1) or future(4)
toCompareWithCurrentClimate = 1
### calculate_distribution_shifts (script 6)
# km resolution for shifted calculations
model_resolution = 0.5
###########################
##### RUNNING SCRIPTS #####
###########################
if (baseline_or_future == 1) {
# get processor ID for R session
worker = paste0(Sys.Date(), "_worker", Sys.getpid())
# create a text file name for specific ongoing processing session
txt_nm = paste0(project_path, "data_input_log_", worker, ".txt")
# write log file in species directory and sink console outputs to log
sink(file(txt_nm, open = "wt"))
# print sign posting for ongoing project run processing
cat('\n', project_run, 'started on ', date(), '\n')
# list data used to keep record of inputs per run
cat('\n Inputs used:', '\n')
cat('species:'); print(all_sp_nm)
cat('species baseline data:', baseData, '\n')
cat('species future data:', futureData, '\n')
cat('bioclimatic variables:', var_names, '\n')
cat('fitting bioclim data:', biofitRun, '\n')
cat('baseline bioclim data:', biobaseRun, '\n')
cat('future bioclim data:', biofutureRun, '\n')
cat('map and crop extent:', map_scale, '\n')
cat('selected models:', models_to_run, '\n')
cat('selected evaluation statistics:', eval_stats, '\n')
cat('number of evalutations and repetitions:', NbRunEval, '&', PA.nb.rep, '\n')
cat('response points (Y)weights used?', useYweights, '\n')
# add any additional notes for project run if needed
cat('\n', 'additional notes: ')
# reset sink from text file to console output
sink(NULL)
}
# start the clock to calculate processing time
ptmStart<-proc.time()
# load necessary packages
require(snowfall)
require(tools)
dataDir
project_path
file.copy(c(paste0(dataDir, "maxent/maxent.jar"), paste0(dataDir, "maxent/maxent.bat")),
project_path, overwrite = TRUE, recursive = FALSE, copy.mode = TRUE)
all_sp_nm
sp_nm=all_sp_nm[1]
sp_nm=all_sp_nm[1]
# load necessary packages
library(biomod2)
library(raster)
library(rgeos)
library(randomForest)
library(dismo)
library(mda)
library(stringr)
library(tools)
require(snowfall)
sp_nm = as.character(sp_nm)
# replace species naming convention of "_" with "."
sp_dir = paste0(str_replace_all(sp_nm,"_", "."), "/")
# create new folder with species name
dir.create(paste0(project_path, sp_dir), showWarnings = FALSE)
temp_sp_files_to_delete<-paste0(project_path, sp_dir, "delete_temp_sp_files/")
temp_sp_files_to_delete
dir.create(temp_sp_files_to_delete, showWarnings = FALSE)
# set temporary directory to created temp file
rasterOptions(tmpdir = temp_sp_files_to_delete)
cat('\n temporary files to be deleted saved here:', temp_sp_files_to_delete, '\n')
cat('\n', sp_nm, 'MODEL FITTING:')
workspace_name = paste0(project_path, sp_dir, sp_nm, "_modelfitting.RData")
FileName00<-paste0(project_path, sp_dir, sp_nm, "_VariImp.csv")
file.exists(FileName00) == FALSE | overwrite == TRUE
cat('\n loading rasters...')
# establish bioclim variable as mask as predictor variable **USE FITTING BIOS***
predictors = raster(paste0(biofitRun, env_var_files[1]))
predictors
for (j in 2:length(env_var_files)){ # add rest bioclim variables to "predictors"
temp = raster(paste0(biofitRun, env_var_files[j]))
#temp = crop(temp, crop_ext, projection = coordSys) #LF disabled as this will take a long time
predictors = addLayer(predictors, temp)
}
names(predictors)<-var_names
rm(j, temp)
plot(predictors, col = rev(terrain.colors(255)), maxpixels = 100000,
useRaster = useRasterDef, axes = TRUE, addfun = NULL)
tiff_name = paste0(project_path, sp_dir, sp_nm, "_env_vars_used.tif")
# create blank image file in working directory
tiff(tiff_name, res = 300, units = "in", pointsize = 12,
width = 10, height = 10, compression = "lzw")
# plot bioclimatic predictors
plot(predictors, col = rev(terrain.colors(255)), maxpixels = 100000,
useRaster = useRasterDef, axes = TRUE, addfun = NULL)
# save image file
dev.off()
# print posting of .tif image saved
cat('\n .tif image file of environmental variables used', tiff_name, 'saved. \n')
# print posting of rasters loaded and saved
cat('\n bioclimatic variables rasters loaded and saved. (Line 110)')
# record time and date stamp
cat(format(Sys.time(), "%a %b %d %Y %X"))
# print posting of loading species point data
cat('\n loading species data...')
# load species occurrence (presence) data
mySpecies<-read.csv(paste0(baseData, sp_nm, ".csv"), header = TRUE)
mySpeciesOcc<-data.frame(mySpecies$decimalLongitude, mySpecies$decimalLatitude)
mySpeciesOcc$pa<-rep(1, length(mySpeciesOcc[,1]))
# rename column headers
names(mySpeciesOcc)<-c("X", "Y", "PA")
# store number of presence points per species
n_Occ_pts<-length(mySpeciesOcc$PA)
# check header of new presence data formatting
head(mySpeciesOcc)
cat('\n defining candidate PA points... (Line 131)')
cat('\n begin selecting points from bioclimatic predictors:')
mySREresp<-reclassify(subset(predictors, 1, drop = TRUE), c(-Inf, Inf, 0))
mySREresp[cellFromXY(mySREresp, mySpeciesOcc[,1:2])]<-1
act_abs = dim(mySpeciesOcc[mySpeciesOcc$PA == 0,])[1]
neg_mySREresp = mySREresp == 0
potential_PAs = rasterToPoints(neg_mySREresp, fun = function(x){x==1})
number_of_PAs
n_Occ_pts
library("roxygen2")
library(devtools)
library("roxygen2")
library(devtools)
repo_dir="D:/code/reproducibility/Rprod/"
dir.create(repo_dir, showWarnings = F)
setwd(repo_dir)
devtools::create("Rprod")
